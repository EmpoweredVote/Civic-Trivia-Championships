---
phase: 17-community-content-generation
plan: 02
type: execute
wave: 2
depends_on: ["17-01"]
files_modified:
  - backend/src/scripts/data/sources/bloomington/*.txt
  - backend/src/db/seed/bloomington-topics.ts
autonomous: false

user_setup:
  - service: anthropic
    why: "AI question generation via Claude API"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (should already exist from Phase 8/12 learning content generation)"

must_haves:
  truths:
    - "Bloomington IN collection has ~100 draft questions in the database"
    - "Questions cover 5-8 locale-specific topic categories at ~70% city/county, ~30% state"
    - "Every question cites an authoritative .gov/.edu/.us source URL in its explanation"
    - "Difficulty distribution matches federal ratio (40% easy, 40% medium, 20% hard)"
    - "Questions about current elected officials have expires_at set to term end dates"
  artifacts:
    - path: "backend/src/scripts/data/sources/bloomington/"
      provides: "RAG source documents from authoritative Bloomington/Indiana .gov sites"
    - path: "database: civic_trivia.questions where external_id LIKE 'bli-%'"
      provides: "~100 Bloomington questions with status='draft'"
    - path: "database: civic_trivia.collection_questions"
      provides: "Junction records linking Bloomington questions to bloomington-in collection"
    - path: "database: civic_trivia.topics"
      provides: "5-8 Bloomington-specific topic records"
  key_links:
    - from: "civic_trivia.questions (bli-* rows)"
      to: "civic_trivia.collection_questions"
      via: "junction table linking to bloomington-in collection"
      pattern: "collectionId matches bloomington-in collection ID"
    - from: "civic_trivia.questions (bli-* rows)"
      to: "civic_trivia.topics"
      via: "topicId foreign key"
      pattern: "each question references a Bloomington-specific topic"
---

<objective>
Fetch Bloomington IN authoritative sources and generate ~100 locale-specific civic trivia questions covering city, county, and state levels.

Purpose: Creates the first community collection with real playable content. Bloomington goes first (per user decision) so the process can be refined before LA.
Output: ~100 draft questions in the database linked to the Bloomington IN collection, with authoritative source citations.
</objective>

<execution_context>
@C:\Users\Chris\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Chris\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-community-content-generation/17-CONTEXT.md
@.planning/phases/17-community-content-generation/17-RESEARCH.md
@.planning/phases/17-community-content-generation/17-01-SUMMARY.md
@backend/src/scripts/content-generation/generate-locale-questions.ts
@backend/src/scripts/content-generation/locale-configs/bloomington-in.ts
@backend/src/scripts/content-generation/question-schema.ts
@backend/src/db/schema.ts
@backend/src/db/seed/collections.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fetch Bloomington RAG sources and create locale topics</name>
  <files>
    backend/src/scripts/data/sources/bloomington/*.txt
  </files>
  <action>
    1. Run the source fetcher to download Bloomington-area authoritative pages:
       ```bash
       cd backend && npx tsx src/scripts/content-generation/generate-locale-questions.ts --locale bloomington-in --fetch-sources
       ```
       This should fetch .gov/.edu pages from bloomingtonConfig.sourceUrls and save cleaned text to backend/src/scripts/data/sources/bloomington/

    2. Review fetched source files â€” ensure they contain substantive civic content:
       - City government structure (mayor, city council, departments)
       - Monroe County government (commissioners, services)
       - Indiana state government (governor, general assembly)
       - Verify at least 5 source documents were fetched successfully

    3. If any key sources failed to fetch (403, timeout, etc.), manually create the missing source files by:
       - Fetching the page content via alternative means (WebFetch tool)
       - Extracting the civic-relevant text
       - Saving to the appropriate .txt file in the sources directory

    4. Run ensureLocaleTopics for Bloomington to create topic records in the database and link them to the bloomington-in collection. This can be done by calling the generation script which handles topic creation automatically, or by creating a small one-off script if the main script doesn't handle this separately.
  </action>
  <verify>
    Check that backend/src/scripts/data/sources/bloomington/ contains at least 5 .txt files with substantive content (each > 500 chars). Verify topics exist in database: query `SELECT t.name FROM civic_trivia.topics t JOIN civic_trivia.collection_topics ct ON t.id = ct.topic_id JOIN civic_trivia.collections c ON ct.collection_id = c.id WHERE c.slug = 'bloomington-in'` returns 5-8 topic rows.
  </verify>
  <done>Bloomington RAG source documents fetched from authoritative .gov/.edu sites. Locale-specific topics created in database and linked to bloomington-in collection.</done>
</task>

<task type="auto">
  <name>Task 2: Generate ~100 Bloomington questions in batches</name>
  <files>
    database: civic_trivia.questions (bli-* rows)
    database: civic_trivia.collection_questions (bloomington links)
  </files>
  <action>
    1. Run the generation script for Bloomington, batch by batch (25 questions per batch, ~4 batches for 100 questions):
       ```bash
       cd backend && npx tsx src/scripts/content-generation/generate-locale-questions.ts --locale bloomington-in
       ```

    2. For each batch, the script will:
       - Call Anthropic API with RAG source documents and prompt caching
       - Validate response against Zod BatchSchema
       - Seed validated questions to database with status='draft'
       - Log cache performance and question distribution

    3. After all batches complete, verify the results:
       - Total question count should be 95-105 (near target of 100)
       - Topic distribution should roughly match config targets
       - Difficulty distribution should be ~40% easy, ~40% medium, ~20% hard
       - All questions should have source URLs from authoritative sites
       - Questions about current officials should have expires_at set

    4. If any batch fails validation or produces poor quality:
       - Check the error output for schema violations
       - Re-run that specific batch: `--batch N`
       - Adjust locale config topic distribution if categories are imbalanced

    5. Run a verification query to confirm data integrity:
       ```sql
       SELECT difficulty, COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'bli-%' GROUP BY difficulty;
       SELECT status, COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'bli-%' GROUP BY status;
       SELECT COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'bli-%' AND source IS NOT NULL;
       ```
  </action>
  <verify>
    Database query `SELECT COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'bli-%'` returns 95-105. Query `SELECT COUNT(*) FROM civic_trivia.collection_questions cq JOIN civic_trivia.collections c ON cq.collection_id = c.id WHERE c.slug = 'bloomington-in'` returns same count. All questions have status='draft', non-null source with valid URL.
  </verify>
  <done>~100 Bloomington IN questions seeded to database as drafts, covering city/county/state civic topics with authoritative source citations and correct difficulty distribution.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>~100 Bloomington IN civic trivia questions generated via AI with RAG sourcing, seeded to database as drafts. Questions cover city government, Monroe County, Indiana state civics, civic history, local services, elections, landmarks, and budget topics.</what-built>
  <how-to-verify>
    1. Review a sample of 10-15 questions across different topics and difficulties
    2. Verify source URLs are accessible and match the claimed facts
    3. Check that distractors are plausible local alternatives (not obviously wrong)
    4. Confirm explanations use "According to [source]..." citation style
    5. Verify elected official questions have reasonable expires_at dates
    6. Check that tone matches federal questions (game-show feel, not dry textbook)
    7. Flag any questions that seem hallucinated, partisan, or factually questionable

    To view questions, run:
    ```bash
    cd backend && npx tsx -e "
      import 'dotenv/config';
      import { db } from './src/db/index.js';
      import { questions } from './src/db/schema.js';
      import { like } from 'drizzle-orm';
      const qs = await db.select().from(questions).where(like(questions.externalId, 'bli-%')).limit(15);
      qs.forEach(q => console.log(JSON.stringify({ id: q.externalId, text: q.text, difficulty: q.difficulty, source: q.source, expiresAt: q.expiresAt }, null, 2)));
      process.exit(0);
    "
    ```
  </how-to-verify>
  <resume-signal>Type "approved" if questions look good, or describe specific issues to fix (e.g., "questions bli-023 and bli-045 have broken source URLs")</resume-signal>
</task>

</tasks>

<verification>
1. backend/src/scripts/data/sources/bloomington/ contains 5+ source documents
2. Database has ~100 questions with external_id matching 'bli-%' pattern
3. All questions have status='draft', non-null source with URL
4. Questions are linked to bloomington-in collection via collection_questions
5. Topic distribution covers 5-8 categories at roughly ~70% city/county, ~30% state
6. Difficulty split is approximately 40/40/20 easy/medium/hard
7. Human reviewer approved question quality
</verification>

<success_criteria>
- Bloomington IN has ~100 draft questions in the database
- Every question has a verifiable source URL from .gov/.edu/.us
- Topic coverage spans city, county, and state levels
- Questions match the game-show tone of federal questions
- Human reviewer approved the question quality
</success_criteria>

<output>
After completion, create `.planning/phases/17-community-content-generation/17-02-SUMMARY.md`
</output>
