---
phase: 17-community-content-generation
plan: 03
type: execute
wave: 3
depends_on: ["17-01", "17-02"]
files_modified:
  - backend/src/scripts/data/sources/los-angeles/*.txt
autonomous: false

must_haves:
  truths:
    - "Los Angeles CA collection has ~100 draft questions in the database"
    - "Questions cover 5-8 locale-specific topic categories at ~70% city/county, ~30% state"
    - "Every question cites an authoritative .gov/.edu/.us source URL in its explanation"
    - "Difficulty distribution matches federal ratio (40% easy, 40% medium, 20% hard)"
    - "Questions about current elected officials have expires_at set to term end dates"
  artifacts:
    - path: "backend/src/scripts/data/sources/los-angeles/"
      provides: "RAG source documents from authoritative LA/California .gov sites"
    - path: "database: civic_trivia.questions where external_id LIKE 'lac-%'"
      provides: "~100 LA questions with status='draft'"
    - path: "database: civic_trivia.collection_questions"
      provides: "Junction records linking LA questions to los-angeles-ca collection"
    - path: "database: civic_trivia.topics"
      provides: "5-8 LA-specific topic records"
  key_links:
    - from: "civic_trivia.questions (lac-* rows)"
      to: "civic_trivia.collection_questions"
      via: "junction table linking to los-angeles-ca collection"
      pattern: "collectionId matches los-angeles-ca collection ID"
    - from: "civic_trivia.questions (lac-* rows)"
      to: "civic_trivia.topics"
      via: "topicId foreign key"
      pattern: "each question references an LA-specific topic"
---

<objective>
Fetch Los Angeles CA authoritative sources and generate ~100 locale-specific civic trivia questions covering city, county, and state levels.

Purpose: Creates the second community collection, applying lessons learned from Bloomington generation.
Output: ~100 draft questions in the database linked to the Los Angeles CA collection, with authoritative source citations.
</objective>

<execution_context>
@C:\Users\Chris\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Chris\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-community-content-generation/17-CONTEXT.md
@.planning/phases/17-community-content-generation/17-01-SUMMARY.md
@.planning/phases/17-community-content-generation/17-02-SUMMARY.md
@backend/src/scripts/content-generation/generate-locale-questions.ts
@backend/src/scripts/content-generation/locale-configs/los-angeles-ca.ts
@backend/src/scripts/content-generation/question-schema.ts
@backend/src/db/schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fetch LA RAG sources and generate ~100 LA questions</name>
  <files>
    backend/src/scripts/data/sources/los-angeles/*.txt
    database: civic_trivia.questions (lac-* rows)
    database: civic_trivia.collection_questions (LA links)
    database: civic_trivia.topics (LA topics)
  </files>
  <action>
    1. Fetch LA source documents:
       ```bash
       cd backend && npx tsx src/scripts/content-generation/generate-locale-questions.ts --locale los-angeles-ca --fetch-sources
       ```

    2. Review fetched sources — ensure substantive civic content for:
       - LA city government (mayor, 15-member city council, departments)
       - LA County (board of supervisors, county services)
       - California state government (governor, legislature, proposition system)
       - If any sources failed, manually fetch via WebFetch and save to los-angeles/ directory

    3. Run full generation:
       ```bash
       cd backend && npx tsx src/scripts/content-generation/generate-locale-questions.ts --locale los-angeles-ca
       ```

    4. Apply any lessons from Bloomington run (documented in 17-02-SUMMARY.md):
       - If prompt adjustments were needed, apply same fixes to LA run
       - If certain topic categories produced poor questions, adjust distribution
       - If batch size needed tuning, use --batch flag accordingly

    5. Verify results with same queries as Bloomington:
       ```sql
       SELECT difficulty, COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'lac-%' GROUP BY difficulty;
       SELECT status, COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'lac-%' GROUP BY status;
       SELECT COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'lac-%' AND source IS NOT NULL;
       ```
  </action>
  <verify>
    Database query `SELECT COUNT(*) FROM civic_trivia.questions WHERE external_id LIKE 'lac-%'` returns 95-105. All questions have status='draft', non-null source. Questions linked to los-angeles-ca collection via collection_questions.
  </verify>
  <done>~100 LA questions seeded to database as drafts, covering city/county/state civic topics with authoritative source citations and correct difficulty distribution.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>~100 Los Angeles CA civic trivia questions generated via AI with RAG sourcing, seeded to database as drafts. Questions cover LA city government, LA County, California state civics, civic history, local services, elections, landmarks, and budget topics.</what-built>
  <how-to-verify>
    1. Review a sample of 10-15 questions across different topics and difficulties
    2. Verify source URLs are accessible and match the claimed facts
    3. Check distractors are plausible local alternatives
    4. Confirm "According to [source]..." citation style in explanations
    5. Verify elected official questions have reasonable expires_at dates
    6. Compare quality against Bloomington questions — should be comparable

    To view questions:
    ```bash
    cd backend && npx tsx -e "
      import 'dotenv/config';
      import { db } from './src/db/index.js';
      import { questions } from './src/db/schema.js';
      import { like } from 'drizzle-orm';
      const qs = await db.select().from(questions).where(like(questions.externalId, 'lac-%')).limit(15);
      qs.forEach(q => console.log(JSON.stringify({ id: q.externalId, text: q.text, difficulty: q.difficulty, source: q.source, expiresAt: q.expiresAt }, null, 2)));
      process.exit(0);
    "
    ```
  </how-to-verify>
  <resume-signal>Type "approved" if questions look good, or describe specific issues to fix</resume-signal>
</task>

</tasks>

<verification>
1. backend/src/scripts/data/sources/los-angeles/ contains 5+ source documents
2. Database has ~100 questions with external_id matching 'lac-%'
3. All questions have status='draft', non-null source with URL
4. Questions linked to los-angeles-ca collection via collection_questions
5. Topic coverage spans city, county, and state levels
6. Difficulty split approximately 40/40/20
7. Human reviewer approved question quality
</verification>

<success_criteria>
- Los Angeles CA has ~100 draft questions in the database
- Every question has a verifiable source URL from .gov/.edu/.us
- Topic coverage spans city, county, and state levels
- Questions match the game-show tone of federal questions
- Human reviewer approved the question quality
</success_criteria>

<output>
After completion, create `.planning/phases/17-community-content-generation/17-03-SUMMARY.md`
</output>
