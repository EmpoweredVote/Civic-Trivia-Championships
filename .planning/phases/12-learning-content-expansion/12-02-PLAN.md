---
phase: 12-learning-content-expansion
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - backend/src/data/questions.json
autonomous: false

must_haves:
  truths:
    - "Learning content coverage reaches 25-30% of question bank (30-36 out of 120 questions)"
    - "New content prioritizes hard-difficulty questions"
    - "Each content piece has 2-3 paragraphs with inline source links"
    - "Each content piece includes answer-specific corrections for wrong options"
    - "Content uses plain language, anti-partisan framing, and as-of-date caveats where needed"
    - "All generated content reviewed by human before application to questions.json"
  artifacts:
    - path: "backend/src/data/questions.json"
      provides: "Question bank with 30-36 questions having learningContent"
      contains: "learningContent"
  key_links:
    - from: "backend/src/scripts/generateLearningContent.ts"
      to: "generated preview JSON file"
      via: "Claude API content generation"
      pattern: "generated-content-.*\\.json"
    - from: "backend/src/scripts/applyContent.ts"
      to: "backend/src/data/questions.json"
      via: "cherry-pick merge of accepted content"
      pattern: "writeFileSync.*questions\\.json"

user_setup:
  - service: anthropic
    why: "Claude API generates learning content"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/settings/keys)"
    dashboard_config: []
---

<objective>
Generate 12-18 new learning content pieces for hard-difficulty questions and apply accepted content to questions.json, reaching 25-30% coverage.

Purpose: This is the core content expansion work. The updated script from Plan 01 generates content in batches with filtering. This plan runs the generation, facilitates human review, and applies accepted content. The target is 30-36 questions with learning content (up from 18).

Output: Updated questions.json with 30-36 questions having learningContent fields.
</objective>

<execution_context>
@C:\Users\Chris\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Chris\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-learning-content-expansion/12-CONTEXT.md
@.planning/phases/12-learning-content-expansion/12-RESEARCH.md
@.planning/phases/12-learning-content-expansion/12-01-SUMMARY.md
@backend/src/scripts/generateLearningContent.ts
@backend/src/scripts/applyContent.ts
@backend/src/data/questions.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate learning content for 15 hard-difficulty questions</name>
  <files>backend/generated-content-*.json (preview file, temporary)</files>
  <action>
Run the updated generateLearningContent.ts script to generate content for a batch of 15 hard-difficulty questions. This targets the largest gap (45 hard questions with no content) with topic spread.

Execute from the backend/ directory:
```bash
cd backend && npx tsx src/scripts/generateLearningContent.ts --difficulty hard --limit 15
```

This will:
1. Filter to the 45 hard questions without learningContent
2. Take the first 15 (which span judiciary, federalism, executive, congress topics per the natural ordering)
3. Generate content via Claude API for each question
4. Write results to a timestamped preview file: `backend/generated-content-{timestamp}.json`

If the script fails partway through (API errors, rate limits), note which questions succeeded and which failed. The retry logic (3 attempts with exponential backoff) should handle transient failures.

After generation completes, print the preview file path for the human review step.

IMPORTANT: This task requires ANTHROPIC_API_KEY to be set in backend/.env or environment. If the key is not available, this task cannot proceed -- inform the user.
  </action>
  <verify>
1. Preview file exists at `backend/generated-content-*.json`
2. Preview file contains 12-15 question entries (some may fail, that's OK)
3. Each entry has: paragraphs (array of 2-3 strings), corrections (object), source (object with name and url)
4. Paragraph text contains inline `[term](url)` links
  </verify>
  <done>
Preview file generated with 12-15 hard-difficulty question content pieces ready for human review.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Generated learning content for ~15 hard-difficulty civics questions. Content is in a preview JSON file (NOT yet applied to questions.json). Each piece has:
- 2-3 paragraphs with inline hyperlinks on key terms
- Answer-specific corrections for each wrong option
- A primary source citation

The content follows the requirements: plain language (8th grade), anti-partisan framing, factual and direct tone, inline links to .gov/educational/Wikipedia sources.
  </what-built>
  <how-to-verify>
1. Open the generated preview file (path will be printed by Task 1)
2. Review each content piece for:
   - **Factual accuracy**: Are the facts correct? Are dates/case names accurate?
   - **Link validity**: Click each inline link and source URL -- do they resolve to real pages?
   - **Tone**: Is it plain language? Anti-partisan? No embellishments?
   - **Corrections**: Does each wrong-answer correction make sense?
   - **Completeness**: Are there 2-3 paragraphs per piece?
3. Note which question IDs to ACCEPT and which to REJECT
4. Tell Claude which IDs to apply (e.g., "apply all" or "apply q076,q078,q080 -- reject q079,q082")
5. If any content needs regeneration, tell Claude which IDs to regenerate

Expected: Most content should be acceptable. Some source URLs may be hallucinated (Claude generates plausible URLs that may 404). Reject those and either fix URLs manually or regenerate.
  </how-to-verify>
  <resume-signal>Provide the list of accepted question IDs (or "apply all"), and list any IDs to reject/regenerate. Example: "Apply all except q079 and q082" or "Apply q076,q078,q080,q083,q084,q086,q087,q089,q091,q092,q094,q097,q109"</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Apply accepted content and verify coverage target</name>
  <files>backend/src/data/questions.json</files>
  <action>
Based on the human review from the checkpoint, apply accepted content using applyContent.ts.

**If "apply all":**
```bash
cd backend && npx tsx src/scripts/applyContent.ts generated-content-{timestamp}.json
```

**If cherry-picking specific IDs:**
```bash
cd backend && npx tsx src/scripts/applyContent.ts generated-content-{timestamp}.json --ids q076,q078,q080,...
```

After applying, verify coverage:
```bash
node --input-type=module << 'EOF'
import { readFileSync } from 'fs';
const q = JSON.parse(readFileSync('backend/src/data/questions.json','utf8'));
const wc = q.filter(x => x.learningContent);
const byDiff = {};
wc.forEach(x => { byDiff[x.difficulty] = (byDiff[x.difficulty]||0)+1; });
console.log(`Coverage: ${wc.length}/${q.length} (${(wc.length/q.length*100).toFixed(1)}%)`);
console.log('By difficulty:', byDiff);
EOF
```

**If coverage is below 30 questions (25%):**
The user may request a second batch. If so, run:
```bash
cd backend && npx tsx src/scripts/generateLearningContent.ts --difficulty hard --limit 10
```
Then repeat the review/apply cycle for the second batch.

**If rejected content needs regeneration:**
Run the script with specific IDs:
```bash
cd backend && npx tsx src/scripts/generateLearningContent.ts --ids q079,q082
```
This creates a new preview file with just those questions. After human review, apply accepted ones.

Target: 30-36 questions with learningContent (25-30% of 120).
  </action>
  <verify>
1. Run coverage check: should show 30-36 questions with content (25-30%)
2. Verify questions.json is valid JSON: `node -e "JSON.parse(require('fs').readFileSync('backend/src/data/questions.json','utf8')); console.log('Valid JSON')"`
3. Spot-check 2-3 newly added entries in questions.json to confirm learningContent structure is correct (has topic, paragraphs, corrections, source)
4. Verify no existing content was modified: the 18 original questions with content should be unchanged
  </verify>
  <done>
questions.json contains 30-36 questions with learningContent (25-30% coverage). All new content has been human-reviewed and accepted. No existing content was modified. Coverage target met.
  </done>
</task>

</tasks>

<verification>
1. Coverage check: 30-36 out of 120 questions have learningContent (25-30%)
2. New content is hard-difficulty questions (priority per CONTEXT.md)
3. Each new content piece has: paragraphs with inline links, corrections, source
4. questions.json is valid JSON
5. Original 18 content pieces are unmodified
6. All content was human-reviewed before application
</verification>

<success_criteria>
- Learning content coverage reaches 25-30% (30-36 questions)
- Hard-difficulty questions prioritized
- All content human-reviewed before merge
- Content includes inline hyperlinks, plain language, anti-partisan framing
- questions.json remains valid and well-formatted
</success_criteria>

<output>
After completion, create `.planning/phases/12-learning-content-expansion/12-02-SUMMARY.md`
</output>
