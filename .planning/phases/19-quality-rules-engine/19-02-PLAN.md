---
phase: 19-quality-rules-engine
plan: 02
type: execute
wave: 2
depends_on: ["19-01"]
files_modified:
  - backend/src/scripts/migrate-quality-score.ts
  - backend/src/db/schema.ts
  - backend/src/scripts/audit-questions.ts
autonomous: true

must_haves:
  truths:
    - "questions table has a quality_score INTEGER column"
    - "Running the audit script produces a console summary with counts and per-collection breakdown"
    - "Running the audit script produces a markdown file listing every question with its score and violations"
    - "The audit script can be re-run at any time (reusable, not one-time)"
  artifacts:
    - path: "backend/src/scripts/migrate-quality-score.ts"
      provides: "Database migration adding quality_score column and index"
      contains: "quality_score"
    - path: "backend/src/scripts/audit-questions.ts"
      provides: "Dry-run audit script producing console + markdown report"
      contains: "auditQuestion"
  key_links:
    - from: "backend/src/scripts/audit-questions.ts"
      to: "backend/src/services/qualityRules/index.ts"
      via: "imports auditQuestion to evaluate each question"
      pattern: "import.*auditQuestion.*qualityRules"
    - from: "backend/src/scripts/audit-questions.ts"
      to: "backend/src/db/schema.ts"
      via: "queries questions and collections from database"
      pattern: "from.*schema"
    - from: "backend/src/scripts/migrate-quality-score.ts"
      to: "backend/src/db/schema.ts"
      via: "adds quality_score column to questions table"
      pattern: "quality_score"
---

<objective>
Add the quality_score column to the database and build the dry-run audit script that evaluates all 320 existing questions against the quality rules engine, producing both a console summary and a detailed markdown report.

Purpose: This delivers QUAL-03 (dry-run audit of all existing questions). The migration also prepares the database for persisting quality scores (needed by Phase 20 admin UI). The audit script is reusable for Phase 21.
Output: Migration script, updated schema, and audit script that generates `audit-report.md`.
</objective>

<execution_context>
@C:\Users\Chris\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Chris\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-quality-rules-engine/19-CONTEXT.md
@.planning/phases/19-quality-rules-engine/19-01-SUMMARY.md
@backend/src/db/schema.ts
@backend/src/scripts/migrate-telemetry.ts
@backend/src/services/questionService.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database migration for quality_score column</name>
  <files>
    backend/src/scripts/migrate-quality-score.ts
    backend/src/db/schema.ts
  </files>
  <action>
    Create `backend/src/scripts/migrate-quality-score.ts` following the same pattern as `migrate-telemetry.ts`:
    - Import `'../env.js'` first (loads environment variables)
    - Import `sql` from drizzle-orm and `db` from `'../db/index.js'`
    - Execute SQL:
      ```sql
      ALTER TABLE civic_trivia.questions
      ADD COLUMN IF NOT EXISTS quality_score INTEGER DEFAULT NULL
      ```
    - Create index:
      ```sql
      CREATE INDEX IF NOT EXISTS idx_questions_quality_score
      ON civic_trivia.questions(quality_score DESC NULLS LAST)
      ```
    - quality_score starts as NULL (not 100) because questions haven't been audited yet. After audit runs, scores get populated. NULL means "not yet scored."
    - Console log success message, then `process.exit(0)`.
    - Wrap in try/catch with `process.exit(1)` on failure.

    Update `backend/src/db/schema.ts`:
    - Add `qualityScore: integer('quality_score')` to the questions table definition (nullable, no default -- matches the migration).
    - Add an index on quality_score in the table's index function: `qualityScoreIdx: index('idx_questions_quality_score').on(table.qualityScore)`

    Run the migration script against the database:
    ```bash
    cd backend && npx tsx src/scripts/migrate-quality-score.ts
    ```
  </action>
  <verify>
    Migration script runs without errors.
    `npx tsc --noEmit` passes (schema.ts compiles with new column).
    Query the database to confirm the column exists: run a quick script or use the existing db connection to `SELECT quality_score FROM civic_trivia.questions LIMIT 1`.
  </verify>
  <done>
    The questions table has a nullable `quality_score` INTEGER column with a descending index. The Drizzle schema reflects this new column. The migration is idempotent (IF NOT EXISTS).
  </done>
</task>

<task type="auto">
  <name>Task 2: Dry-run audit script with console and markdown output</name>
  <files>
    backend/src/scripts/audit-questions.ts
  </files>
  <action>
    Create `backend/src/scripts/audit-questions.ts`:
    - Import `'../env.js'` first
    - Import `db` from `'../db/index.js'`
    - Import `questions, collections, collectionQuestions` from `'../db/schema.js'`
    - Import `auditQuestion` from `'../services/qualityRules/index.js'`
    - Import `eq, sql` from `'drizzle-orm'`
    - Import `writeFileSync` from `'fs'`

    Main function `runAudit()`:
    1. Accept optional CLI flags:
       - `--skip-url-check` to skip Learn More link validation (fast mode)
       - `--save-scores` to persist quality_score to database after audit
    2. Parse `process.argv` for flags

    3. Fetch all active questions with their collection memberships:
       ```
       SELECT q.*, array_agg(c.name) as collection_names, array_agg(c.id) as collection_ids
       FROM questions q
       JOIN collection_questions cq ON q.id = cq.question_id
       JOIN collections c ON cq.collection_id = c.id
       WHERE q.status = 'active'
       GROUP BY q.id
       ```
       Use Drizzle query builder or raw SQL for the aggregation.

    4. Run `auditQuestion()` on each question (with `skipUrlCheck` flag from CLI).
       - If NOT skipping URL checks, process in batches of 10 to avoid hammering servers.
       - Show progress: "Auditing question X/320..." every 50 questions.

    5. Console summary output:
       ```
       === QUALITY AUDIT REPORT ===

       Total questions audited: 320
       Passing all rules: XXX
       Blocking violations (would archive): XX
       Advisory only (flagged for review): XX

       --- Per-Collection Breakdown ---
       Collection Name          | Total | Pass | Block | Advisory | After Archival | Status
       Federal Civics           |   100 |   85 |     5 |       10 |             95 | OK (>= 50)
       Denver Metro             |    80 |   70 |     3 |        7 |             77 | OK (>= 50)
       ...

       --- Blocking Violation Summary ---
       ambiguous-answers: XX questions
       vague-qualifiers: XX questions
       pure-lookup: XX questions
       broken-learn-more: XX questions

       --- Advisory Violation Summary ---
       weak-explanation: XX questions
       short-question: XX questions
       partisan-framing: XX questions
       ...
       ```

    6. Generate markdown report file at `backend/audit-report.md`:
       ```markdown
       # Question Quality Audit Report

       **Generated:** [ISO timestamp]
       **Total Questions:** 320
       **Mode:** [full / skip-url-check]

       ## Summary

       | Metric | Count |
       |--------|-------|
       | Passing all rules | XXX |
       | Blocking violations | XX |
       | Advisory only | XX |

       ## Collection Impact

       | Collection | Total | Pass | Block | Advisory | After Archival | Status |
       |------------|-------|------|-------|----------|----------------|--------|
       | ... | ... | ... | ... | ... | ... | OK/WARNING/CRITICAL |

       Status key:
       - OK: >= 50 active questions after archival
       - WARNING: collection would be hidden (< 50 after archival)

       ## Questions to Archive (Blocking Violations)

       ### [externalId]: [question text truncated to 80 chars]...
       **Collection(s):** [collection names]
       **Score:** XX/100
       **Blocking violations:**
       - [rule] (blocking): [message] — Evidence: [evidence]

       ## Questions Flagged (Advisory Only)

       ### [externalId]: [question text truncated to 80 chars]...
       **Collection(s):** [collection names]
       **Score:** XX/100
       **Advisory violations:**
       - [rule] (advisory): [message] — Evidence: [evidence]

       ## All Questions by Score (Ascending)

       | Rank | ID | Score | Blocking | Advisory | Collection(s) |
       |------|-----|-------|----------|----------|---------------|
       | 1 | q042 | 25 | 2 | 1 | Federal Civics |
       | ... |
       ```

    7. If `--save-scores` flag is present, update quality_score in the database for each audited question:
       ```typescript
       for (const result of auditResults) {
         await db.update(questions)
           .set({ qualityScore: result.score })
           .where(eq(questions.externalId, result.question.externalId));
       }
       ```
       Log: "Saved quality scores to database for XXX questions"

    8. Exit with `process.exit(0)`.

    Add npm script to `backend/package.json`:
    ```json
    "audit-questions": "tsx src/scripts/audit-questions.ts"
    ```
  </action>
  <verify>
    Run `npx tsc --noEmit` -- script compiles.
    Run `npx tsx src/scripts/audit-questions.ts --skip-url-check` from backend directory.
    Confirm console output shows audit summary with counts and per-collection breakdown.
    Confirm `backend/audit-report.md` file is created with full report.
    Run again with `--save-scores` and verify quality_score column is populated (spot-check a few rows).
  </verify>
  <done>
    The audit script evaluates all 320 active questions against quality rules, produces a console summary with per-collection breakdown and violation counts, generates a detailed markdown report at `backend/audit-report.md`, and optionally persists scores to the database. The script is reusable (can be re-run at any time with the same or different flags).
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes in backend directory
2. `quality_score` column exists in database with descending index
3. `backend/src/db/schema.ts` includes `qualityScore` field
4. Running `npm run audit-questions -- --skip-url-check` produces console summary and `audit-report.md`
5. Running with `--save-scores` persists scores to database
6. Report shows per-collection breakdown with "after archival" counts and OK/WARNING status
</verification>

<success_criteria>
- Migration adds quality_score column (nullable integer) with index
- Audit script evaluates all active questions against quality rules
- Console output shows total counts, per-collection breakdown, and violation summaries
- Markdown report lists every question with blocking violations first, then advisory, then all by score
- Script is reusable via `npm run audit-questions` with optional flags
- Scores can optionally be saved to database
</success_criteria>

<output>
After completion, create `.planning/phases/19-quality-rules-engine/19-02-SUMMARY.md`
</output>
